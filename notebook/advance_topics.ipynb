{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66fc06d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. pytest \n",
    "# 2. cache\n",
    "# 3. pydantic object and output parser\n",
    "# 4. token counter(cost analysis)\n",
    "# 5. history or memory \n",
    "# 6. evaluation\n",
    "# 7. guadrails\n",
    "# 8. adavance RAG\n",
    "\n",
    "## as a experiment will do in the notebook\n",
    "### then as a assignment you can integrate it in end to end project\n",
    "\n",
    "## Next Saturday there will demonstration of this concept (whoever will comeplete it\n",
    "# that person will give the demo)\n",
    "\n",
    "### next week satuday 2-2:30\n",
    "\n",
    "### then will start with the next project\n",
    "\n",
    "#For first winner there is prize money:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49527aa2",
   "metadata": {},
   "source": [
    "Assignment on first Project:\n",
    "1. you have to make enable this project for every document(.ppt,.docx,.md,.txt,.pdf,.xlxs,.csv,anysqldb)\n",
    "2. you have to add a code for dealing with table and images data also\n",
    "3. you have to add the evalation matrix using the DeepEval\n",
    "4. write at least 10 test cases\n",
    "5. this cases should be vaildate before the commit and after the commit\n",
    "6. add one login screen to your protal\n",
    "7. use the langchain inmemory cache and implement inside the project\n",
    "8. deadline for this assignment is till 5th of september(friday)\n",
    "9. inner will present the entire solution to the whole class (30 minute presentation would be there)\n",
    "10. prize: money or course or giftpack(there will be only one winnner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27fb2fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from utils.model_loader import ModelLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d950afbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"timestamp\": \"2025-10-22T01:24:18.929345Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-22T01:24:18.930023Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-22T01:24:18.931111Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_Dm...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-22T01:24:18.932029Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"faiss_db\", \"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-22T01:24:18.935342Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n"
     ]
    }
   ],
   "source": [
    "loader = ModelLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6794aed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"provider\": \"google\", \"model\": \"gemini-2.0-flash\", \"timestamp\": \"2025-10-22T01:24:21.537056Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Loaded: model='models/gemini-2.0-flash' google_api_key=SecretStr('**********') temperature=0.0 max_output_tokens=2048 client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x107c347d0> default_metadata=() model_kwargs={}\n",
      "LLM Result: I am doing well, thank you for asking! How are you today?\n"
     ]
    }
   ],
   "source": [
    "llm = loader.load_llm()\n",
    "print(f\"LLM Loaded: {llm}\")\n",
    "result = llm.invoke(\"Hello, how are you?\")\n",
    "print(f\"LLM Result: {result.content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eeb6a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd17a47e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Return a JSON object.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb10b887",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    \"Give me a JSON with keys 'title' and 'summary' for this topic: {topic}\\n{format_instructions}\"\n",
    ").partial(format_instructions=parser.get_format_instructions())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7121d8bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={'format_instructions': 'Return a JSON object.'}, template=\"Give me a JSON with keys 'title' and 'summary' for this topic: {topic}\\n{format_instructions}\")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff7a92f",
   "metadata": {},
   "source": [
    "PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={'format_instructions': 'Return a JSON object.'}, template=\"Give me a JSON with keys 'title' and 'summary' for this topic: {topic}\\n{format_instructions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a26281ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9ec5cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({\"topic\": \"LangChain for RAG\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abf7461f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'LangChain for Retrieval Augmented Generation (RAG)',\n",
       " 'summary': 'LangChain simplifies the development of Retrieval Augmented Generation (RAG) applications. It provides modules for connecting to data sources, creating embeddings, indexing data, and building chains that combine retrieval and generation models. This allows developers to build powerful applications that leverage external knowledge to enhance the quality and relevance of generated text.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9b2901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import OutputFixingParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5503f693",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_parser = JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0d8be96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing wrapper\n",
    "fixing_parser = OutputFixingParser.from_llm(parser=json_parser, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "551fc59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broken JSON\n",
    "bad_output = \"\"\"\n",
    "title: LangChain RAG\n",
    "summary LangChain helps with retrieval augmented generation...\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b20efef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fixed_result = fixing_parser.parse(bad_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "542c0764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'LangChain RAG', 'summary': 'LangChain helps with retrieval augmented generation...'}\n"
     ]
    }
   ],
   "source": [
    "print(fixed_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee98b00a",
   "metadata": {},
   "source": [
    "| Feature                | `JsonOutputParser`               | `OutputFixingParser`                     \n",
    "| ---------------------- | -------------------------------- | ---------------------------------------- \n",
    "| Purpose                | Enforce strict JSON              | Auto-correct bad/invalid output          \n",
    "| Works with             | Well-formed model responses only | Even with malformed model responses      \n",
    "| Raises error if broken | Yes                            |   No, it tries to fix using LLM          \n",
    "| Ideal for              | Strict structured output         | Fragile prompts, semi-structured outputs \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69055cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.output_parsers import OutputFixingParser\n",
    "# from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# json_parser = JsonOutputParser()\n",
    "# safe_parser = OutputFixingParser.from_llm(parser=json_parser, llm=llm)\n",
    "\n",
    "# # Use safe_parser instead of json_parser\n",
    "# chain = prompt | llm | safe_parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcb4cab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap with cache\n",
    "Model_Cache = {}\n",
    "\n",
    "import time\n",
    "def cached_model(query):\n",
    "    start_time = time.time()\n",
    "    if Model_Cache.get(query):\n",
    "        print(\"***CACHE HIT***\")\n",
    "        end_time = time.time()\n",
    "        elapsed = end_time - start_time\n",
    "        print(f\"EXECUTION TIME: {elapsed:.2f} seconds\")\n",
    "        return Model_Cache.get(query)\n",
    "    else:\n",
    "        print(\"***CACHE MISS – EXECUTING MODEL***\")\n",
    "        start_time = time.time()\n",
    "        response = llm.invoke(query)\n",
    "        end_time = time.time()\n",
    "        elapsed = end_time - start_time\n",
    "        print(f\"EXECUTION TIME: {elapsed:.2f} seconds\")\n",
    "        Model_Cache[query] = response\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8832440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***CACHE MISS – EXECUTING MODEL***\n",
      "EXECUTION TIME: 0.56 seconds\n",
      "content='Hi there! How can I help you today?' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run--9a568ffa-754a-4d67-b034-2c06828f4919-0' usage_metadata={'input_tokens': 1, 'output_tokens': 11, 'total_tokens': 12, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "query=\"hi\"\n",
    "response = cached_model(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03d69528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***CACHE MISS – EXECUTING MODEL***\n",
      "EXECUTION TIME: 8.94 seconds\n",
      "content=\"## The Enduring Quest for Independence: A Multifaceted Exploration\\n\\nIndependence, a word that resonates with power and promise, is a concept deeply woven into the fabric of human existence. It signifies the state of being free from the control, influence, or support of others, allowing for self-governance, autonomy, and the pursuit of one's own path. While often associated with political liberation and national sovereignty, independence extends far beyond the realm of nations, encompassing personal, economic, intellectual, and even spiritual dimensions. This essay will explore the multifaceted nature of independence, examining its historical significance, its psychological underpinnings, its economic implications, and the challenges and responsibilities that accompany its attainment.\\n\\nHistorically, the pursuit of independence has been a driving force behind countless revolutions and social movements. From the American Revolution, where colonists sought freedom from British rule, to the Indian independence movement led by Mahatma Gandhi, the desire for self-determination has fueled struggles against oppression and injustice. These movements were not merely about political autonomy; they were about reclaiming the right to define one's own destiny, to shape one's own culture, and to control one's own resources. The legacy of these struggles continues to inspire movements for self-determination around the world, reminding us that the fight for independence is an ongoing process, requiring vigilance and a commitment to justice.\\n\\nThe psychological significance of independence is equally profound. From a young age, individuals strive for autonomy, seeking to break free from the constraints of parental control and establish their own identities. This process of individuation is crucial for developing a sense of self-worth, self-reliance, and the ability to make independent decisions. Erik Erikson, a renowned developmental psychologist, identified autonomy versus shame and doubt as a critical stage in early childhood development. Successfully navigating this stage allows individuals to develop a sense of independence and confidence in their abilities, while failure can lead to feelings of inadequacy and dependence.\\n\\nFurthermore, the pursuit of independence fosters creativity and innovation. When individuals are free to explore their own ideas and pursue their own passions, they are more likely to challenge conventional wisdom and develop novel solutions to complex problems. The history of scientific and artistic progress is replete with examples of individuals who dared to think independently, challenging established norms and pushing the boundaries of human knowledge and expression. From Galileo Galilei, who challenged the geocentric view of the universe, to Marie Curie, who pioneered research on radioactivity, independent thinkers have consistently driven progress and shaped the world we live in.\\n\\nEconomic independence is another crucial aspect of this multifaceted concept. It refers to the ability to support oneself financially, without relying on external assistance. This can be achieved through employment, entrepreneurship, or investment. Economic independence empowers individuals to make their own choices about their lives, to pursue their own goals, and to contribute to the economic well-being of their communities. It also reduces vulnerability to exploitation and dependence on others, fostering a sense of security and self-sufficiency.\\n\\nHowever, the pursuit of economic independence is not without its challenges. Systemic inequalities, such as discrimination based on race, gender, or socioeconomic status, can create significant barriers to economic opportunity. Access to education, healthcare, and affordable housing are essential prerequisites for achieving economic independence, and addressing these inequalities is crucial for creating a more just and equitable society. Furthermore, the rise of automation and the changing nature of work pose new challenges to economic independence, requiring individuals to adapt to new skills and embrace lifelong learning.\\n\\nIntellectual independence, the ability to think critically and form one's own opinions, is perhaps the most fundamental aspect of independence. It involves questioning assumptions, evaluating evidence, and resisting the pressure to conform to popular opinion. Intellectual independence is essential for informed decision-making, responsible citizenship, and the pursuit of truth. It requires a commitment to lifelong learning, a willingness to engage in respectful dialogue, and the courage to challenge prevailing narratives.\\n\\nIn an age of information overload and pervasive propaganda, intellectual independence is more important than ever. The ability to discern fact from fiction, to identify bias, and to critically evaluate sources of information is crucial for navigating the complexities of the modern world. Cultivating intellectual independence requires developing strong critical thinking skills, fostering a healthy skepticism, and embracing intellectual humility. It also requires resisting the temptation to rely solely on echo chambers and seeking out diverse perspectives.\\n\\nSpiritual independence, while often overlooked, is a vital component of overall well-being. It refers to the freedom to explore one's own beliefs and values, without being constrained by dogma or external pressure. Spiritual independence allows individuals to connect with something larger than themselves, to find meaning and purpose in their lives, and to develop a strong moral compass. It does not necessarily imply a rejection of organized religion, but rather a commitment to personal exploration and authentic self-expression.\\n\\nThe pursuit of spiritual independence can be a challenging but ultimately rewarding journey. It requires introspection, self-reflection, and a willingness to question one's own assumptions. It also requires embracing uncertainty and accepting that there may be no easy answers to life's big questions. However, the rewards of spiritual independence are immense, including a greater sense of inner peace, a deeper connection to oneself and others, and a more meaningful and fulfilling life.\\n\\nWhile independence is a desirable goal, it is important to recognize that it is not an absolute state. We are all interconnected and interdependent, relying on others for support, collaboration, and companionship. True independence is not about isolating oneself from others, but rather about developing the capacity to make informed choices and to live authentically, while also contributing to the well-being of the community.\\n\\nFurthermore, independence comes with responsibilities. With the freedom to make our own choices comes the obligation to consider the consequences of those choices and to act in a responsible and ethical manner. Independent individuals are accountable for their actions and are expected to contribute to the common good. They are also expected to respect the rights and freedoms of others, even when those rights and freedoms conflict with their own.\\n\\nIn conclusion, independence is a multifaceted concept that encompasses political, psychological, economic, intellectual, and spiritual dimensions. It is a fundamental human aspiration that has driven countless revolutions and social movements throughout history. While the pursuit of independence is not without its challenges, the rewards are immense, including greater self-reliance, autonomy, and the ability to live a more meaningful and fulfilling life. However, true independence is not about isolation, but rather about developing the capacity to make informed choices and to live authentically, while also contributing to the well-being of the community. As we navigate the complexities of the modern world, the enduring quest for independence remains a vital and essential pursuit. It is a journey that requires courage, resilience, and a commitment to justice, but one that ultimately leads to a more just, equitable, and fulfilling world for all.\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run--db41e85a-daa0-4b79-affe-8ad774ff1bfb-0' usage_metadata={'input_tokens': 14, 'output_tokens': 1382, 'total_tokens': 1396, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "query=\"can you give me 1000 words essay on independence?\"\n",
    "response = cached_model(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b9da868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***CACHE HIT***\n",
      "EXECUTION TIME: 0.00 seconds\n",
      "content=\"## The Enduring Quest for Independence: A Multifaceted Exploration\\n\\nIndependence, a word that resonates with power and promise, is a concept deeply woven into the fabric of human existence. It signifies the state of being free from the control, influence, or support of others, allowing for self-governance, autonomy, and the pursuit of one's own path. While often associated with political liberation and national sovereignty, independence extends far beyond the realm of nations, encompassing personal, economic, intellectual, and even spiritual dimensions. This essay will explore the multifaceted nature of independence, examining its historical significance, its psychological underpinnings, its economic implications, and the challenges and responsibilities that accompany its attainment.\\n\\nHistorically, the pursuit of independence has been a driving force behind countless revolutions and social movements. From the American Revolution, where colonists sought freedom from British rule, to the Indian independence movement led by Mahatma Gandhi, the desire for self-determination has fueled struggles against oppression and injustice. These movements were not merely about political autonomy; they were about reclaiming the right to define one's own destiny, to shape one's own culture, and to control one's own resources. The legacy of these struggles continues to inspire movements for self-determination around the world, reminding us that the fight for independence is an ongoing process, requiring vigilance and a commitment to justice.\\n\\nThe psychological significance of independence is equally profound. From a young age, individuals strive for autonomy, seeking to break free from the constraints of parental control and establish their own identities. This process of individuation is crucial for developing a sense of self-worth, self-reliance, and the ability to make independent decisions. Erik Erikson, a renowned developmental psychologist, identified autonomy versus shame and doubt as a critical stage in early childhood development. Successfully navigating this stage allows individuals to develop a sense of independence and confidence in their abilities, while failure can lead to feelings of inadequacy and dependence.\\n\\nFurthermore, the pursuit of independence fosters creativity and innovation. When individuals are free to explore their own ideas and pursue their own passions, they are more likely to challenge conventional wisdom and develop novel solutions to complex problems. The history of scientific and artistic progress is replete with examples of individuals who dared to think independently, challenging established norms and pushing the boundaries of human knowledge and expression. From Galileo Galilei, who challenged the geocentric view of the universe, to Marie Curie, who pioneered research on radioactivity, independent thinkers have consistently driven progress and shaped the world we live in.\\n\\nEconomic independence is another crucial aspect of this multifaceted concept. It refers to the ability to support oneself financially, without relying on external assistance. This can be achieved through employment, entrepreneurship, or investment. Economic independence empowers individuals to make their own choices about their lives, to pursue their own goals, and to contribute to the economic well-being of their communities. It also reduces vulnerability to exploitation and dependence on others, fostering a sense of security and self-sufficiency.\\n\\nHowever, the pursuit of economic independence is not without its challenges. Systemic inequalities, such as discrimination based on race, gender, or socioeconomic status, can create significant barriers to economic opportunity. Access to education, healthcare, and affordable housing are essential prerequisites for achieving economic independence, and addressing these inequalities is crucial for creating a more just and equitable society. Furthermore, the rise of automation and the changing nature of work pose new challenges to economic independence, requiring individuals to adapt to new skills and embrace lifelong learning.\\n\\nIntellectual independence, the ability to think critically and form one's own opinions, is perhaps the most fundamental aspect of independence. It involves questioning assumptions, evaluating evidence, and resisting the pressure to conform to popular opinion. Intellectual independence is essential for informed decision-making, responsible citizenship, and the pursuit of truth. It requires a commitment to lifelong learning, a willingness to engage in respectful dialogue, and the courage to challenge prevailing narratives.\\n\\nIn an age of information overload and pervasive propaganda, intellectual independence is more important than ever. The ability to discern fact from fiction, to identify bias, and to critically evaluate sources of information is crucial for navigating the complexities of the modern world. Cultivating intellectual independence requires developing strong critical thinking skills, fostering a healthy skepticism, and embracing intellectual humility. It also requires resisting the temptation to rely solely on echo chambers and seeking out diverse perspectives.\\n\\nSpiritual independence, while often overlooked, is a vital component of overall well-being. It refers to the freedom to explore one's own beliefs and values, without being constrained by dogma or external pressure. Spiritual independence allows individuals to connect with something larger than themselves, to find meaning and purpose in their lives, and to develop a strong moral compass. It does not necessarily imply a rejection of organized religion, but rather a commitment to personal exploration and authentic self-expression.\\n\\nThe pursuit of spiritual independence can be a challenging but ultimately rewarding journey. It requires introspection, self-reflection, and a willingness to question one's own assumptions. It also requires embracing uncertainty and accepting that there may be no easy answers to life's big questions. However, the rewards of spiritual independence are immense, including a greater sense of inner peace, a deeper connection to oneself and others, and a more meaningful and fulfilling life.\\n\\nWhile independence is a desirable goal, it is important to recognize that it is not an absolute state. We are all interconnected and interdependent, relying on others for support, collaboration, and companionship. True independence is not about isolating oneself from others, but rather about developing the capacity to make informed choices and to live authentically, while also contributing to the well-being of the community.\\n\\nFurthermore, independence comes with responsibilities. With the freedom to make our own choices comes the obligation to consider the consequences of those choices and to act in a responsible and ethical manner. Independent individuals are accountable for their actions and are expected to contribute to the common good. They are also expected to respect the rights and freedoms of others, even when those rights and freedoms conflict with their own.\\n\\nIn conclusion, independence is a multifaceted concept that encompasses political, psychological, economic, intellectual, and spiritual dimensions. It is a fundamental human aspiration that has driven countless revolutions and social movements throughout history. While the pursuit of independence is not without its challenges, the rewards are immense, including greater self-reliance, autonomy, and the ability to live a more meaningful and fulfilling life. However, true independence is not about isolation, but rather about developing the capacity to make informed choices and to live authentically, while also contributing to the well-being of the community. As we navigate the complexities of the modern world, the enduring quest for independence remains a vital and essential pursuit. It is a journey that requires courage, resilience, and a commitment to justice, but one that ultimately leads to a more just, equitable, and fulfilling world for all.\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run--db41e85a-daa0-4b79-affe-8ad774ff1bfb-0' usage_metadata={'input_tokens': 14, 'output_tokens': 1382, 'total_tokens': 1396, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "query=\"can you give me 1000 words essay on independence?\"\n",
    "response = cached_model(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc9aa9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"model\": \"models/text-embedding-004\", \"timestamp\": \"2025-10-22T01:25:29.919964Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Model Loaded: client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x111224590> async_client=<google.ai.generativelanguage_v1beta.services.generative_service.async_client.GenerativeServiceAsyncClient object at 0x1112203b0> model='models/text-embedding-004' task_type=None google_api_key=SecretStr('**********') credentials=None client_options=None transport=None request_options=None\n",
      "Embedding Result: [0.012455824762582779, -0.022220758721232414, -0.050555989146232605, -0.04617856815457344, 0.007134224288165569, 0.03131191432476044, 0.03413437679409981, 0.002971651265397668, -0.010995007120072842, 0.04237418621778488, -0.01806010864675045, 0.02493678405880928, 0.1121494323015213, -0.013102629221975803, -0.0148635134100914, -0.024023527279496193, 0.010264375247061253, -0.0037090936675667763, -0.10687677562236786, -0.001329770078882575, 0.03868139907717705, -0.03769991546869278, 0.03614252060651779, 0.005920442286878824, -0.04356273636221886, -0.004675108939409256, -0.023465346544981003, -0.010386619716882706, 0.011209390126168728, -0.025817539542913437, 0.04684286192059517, 0.08373904973268509, 0.010084772482514381, -0.03376094251871109, 0.020884841680526733, 0.02289341762661934, -0.003998701926320791, 0.01330992579460144, 0.0509757325053215, -0.09198947250843048, -0.07254136353731155, 0.04478190466761589, -0.0014892473118379712, 0.03480713814496994, -0.002893605502322316, -0.021368319168686867, 0.026944760233163834, 0.03481404110789299, -0.0113154212012887, 0.04239237308502197, 0.028903041034936905, 0.022403204813599586, -0.05002392828464508, 0.006107809022068977, -0.006879379041492939, -0.0005574015085585415, 0.00018034980166703463, 0.008374546654522419, 0.04145769402384758, -0.025920633226633072, 0.00581536628305912, 0.007453762460500002, -0.015612686984241009, -0.03611474484205246, 0.014897655695676804, -0.037706803530454636, 0.011048460379242897, -0.005773257464170456, -0.07338972389698029, 0.041947416961193085, -0.008714448660612106, 0.013749705627560616, -0.05058510601520538, 0.008701021783053875, -0.026075800880789757, -0.010403540916740894, -0.010278790257871151, -0.005401624366641045, 0.011471167206764221, 0.033857714384794235, -0.04043952003121376, 0.015613697469234467, 0.06003198027610779, 0.07738664001226425, -0.023364201188087463, 0.029591014608740807, 0.01976163499057293, -0.06802070140838623, -0.05220983177423477, -0.010016380809247494, 0.10859932005405426, 0.011279468424618244, 0.007659756112843752, -0.015049867331981659, 0.06771127879619598, -0.01944717764854431, -0.08944784849882126, -0.10393165796995163, 0.11675474792718887, 0.007203542161732912, 0.0008657867438159883, 0.007421628572046757, -0.009653370827436447, -0.06819561868906021, 0.01945577934384346, 0.051638804376125336, -0.029942573979496956, -0.042958736419677734, -0.019864805042743683, 0.017653953284025192, -0.017503488808870316, -0.044733524322509766, 0.03494996204972267, -0.0019917881581932306, -0.007429750170558691, 0.023747682571411133, -0.0431840755045414, 0.0019314548699185252, -0.03294016048312187, 0.014468812383711338, -0.0024643908254802227, 0.025144264101982117, -0.022134527564048767, 0.10755819827318192, 0.02490936405956745, 0.011192023754119873, 0.0015716323396191, -0.02178562432527542, -0.06771239638328552, -0.049380138516426086, 0.08783523738384247, -0.023269586265087128, 0.009737566113471985, 0.032387129962444305, -0.052026230841875076, -0.01877208985388279, 0.06508863717317581, -0.05382818356156349, 0.039945654571056366, 0.014005248434841633, -0.004315309226512909, -0.05164776369929314, -0.039541348814964294, 0.00784064270555973, 0.008627673611044884, -0.018560588359832764, -0.01660410314798355, 0.06843478977680206, -0.04986061155796051, -0.009544438682496548, -0.043490778654813766, -0.01982610858976841, 0.05358990281820297, -0.05194596201181412, 0.00924274418503046, 0.022442573681473732, 0.054759472608566284, -0.03969776630401611, 0.04876459762454033, -0.012862351723015308, 0.04954301565885544, -0.07955924421548843, -0.04404660686850548, 0.015906522050499916, -0.01791670173406601, 0.005397598259150982, 0.0038055086042732, -0.036019161343574524, 0.013881949707865715, 0.016763104125857353, -0.034556448459625244, -0.05342577397823334, -0.01381685771048069, -0.15006470680236816, -0.019942589104175568, 0.002566175302490592, -0.02811739221215248, -0.020092783495783806, -0.015126613900065422, -0.018085237592458725, 0.10519368946552277, 0.021229058504104614, -0.019142627716064453, -0.07908312976360321, 0.006924382410943508, -0.015233058482408524, 0.026185274124145508, 0.024732710793614388, 0.060259416699409485, 0.041969891637563705, -0.026774421334266663, 0.00799805298447609, 0.011700641363859177, 0.0342189259827137, -0.03183373063802719, 0.011145621538162231, 0.03164193034172058, -0.04771155118942261, 0.026175597682595253, -0.0489879809319973, 0.06313765048980713, 0.020891299471259117, -0.020877251401543617, -0.012239369563758373, -0.030345018953084946, 0.03507860377430916, -0.05327361077070236, -0.07006189227104187, -0.025967419147491455, 0.006272461265325546, -0.018474698066711426, 0.01101798377931118, -0.026081660762429237, -0.04022398963570595, 0.037948720157146454, -0.00023168617917690426, 0.0676269382238388, -0.005176514387130737, 0.01971936598420143, -0.03912899270653725, 0.049724530428647995, 0.05640000104904175, 0.036826711148023605, 0.023012086749076843, 0.023907935246825218, -0.003595447400584817, -0.05384625121951103, 0.0001777269208105281, 0.009188906289637089, -0.05081622675061226, 0.011230272240936756, 0.016221387311816216, -0.010362103581428528, 0.030504271388053894, -0.06813428550958633, 0.045014530420303345, 0.046261951327323914, -0.019377881661057472, -0.011723337695002556, 0.012803950347006321, -0.030391035601496696, -0.0030907855834811926, 0.06818464398384094, 0.01678100973367691, 0.04427051916718483, -0.04993132874369621, 0.06181267648935318, 0.05030204355716705, 0.022441618144512177, -0.025567203760147095, -0.015064483508467674, -0.029366008937358856, -0.015265165828168392, 0.0031529932748526335, -0.08744263648986816, -0.016554730013012886, 0.033491041511297226, -0.004213474690914154, 0.0006747462321072817, -0.032266318798065186, 0.07835523039102554, -0.013851643539965153, -0.005762961693108082, -0.10156656801700592, -0.03644723445177078, -0.07768076658248901, -0.01030147634446621, 0.013028323650360107, 0.027633754536509514, -0.05577288940548897, 0.013342211954295635, -0.017482010647654533, -0.0411427803337574, 0.007647169288247824, -0.011470891535282135, 0.02215852402150631, 0.011031962931156158, 0.004099421203136444, -0.014440973289310932, -0.018185188993811607, 0.009319433942437172, 0.004363721702247858, 0.004475305322557688, 0.005963848903775215, 0.0012982389889657497, -0.07796481996774673, 0.003906753845512867, 0.023633567616343498, -0.027058854699134827, -0.000518499466124922, 0.05265210568904877, 0.05519836023449898, -0.0343669094145298, -0.02824120968580246, 0.016249554231762886, 0.0033043352887034416, 0.03540894016623497, 0.033357247710227966, -0.008044273592531681, -0.0029608916956931353, 0.024958647787570953, 0.050681307911872864, -0.037793662399053574, 0.04992763698101044, 0.0032780796755105257, -0.009910456836223602, -0.029421841725707054, -0.0147116519510746, -0.021794697269797325, 0.03465849161148071, -0.036990948021411896, 0.03658265620470047, -0.030192671343684196, -0.0320877879858017, -0.03083866462111473, -0.01527484506368637, -0.16792982816696167, -0.013539016246795654, -0.0018505020998418331, -0.01373943779617548, 0.019737526774406433, 0.02806883677840233, -0.01974073424935341, 0.0017742604250088334, -0.0052481759339571, -0.01596209593117237, 0.007207742426544428, -0.007615270093083382, 0.006253538653254509, 0.01837211474776268, 0.012095113284885883, 0.005751940421760082, -0.04393688589334488, -0.0458187460899353, 0.016736146062612534, 0.026165161281824112, -0.06021394580602646, 0.012089228257536888, 0.06863582134246826, 0.055838871747255325, 0.0484868586063385, 0.04481371119618416, 0.04715169966220856, 0.01655426062643528, -0.020898090675473213, -0.05972684919834137, -0.053977467119693756, 0.007847784087061882, 0.03233913332223892, 0.010286054573953152, 0.018272725865244865, 0.05693472921848297, 0.026759905740618706, -0.053598854690790176, -0.01714101992547512, -0.017183609306812286, 0.055396780371665955, 0.0011198350694030523, 0.027947822585701942, 0.004425154998898506, 0.011749014258384705, 0.0153060806915164, 0.005009829066693783, 0.010540829040110111, 0.048252787441015244, -0.006977658718824387, 0.011299684643745422, 0.03948843851685524, -0.009677493944764137, -0.05354953929781914, -0.008452726528048515, 0.00911550410091877, -0.01360390242189169, -0.025395328179001808, 0.031119374558329582, -0.01998763158917427, -0.05207986384630203, 0.016682274639606476, -0.015195782296359539, 0.017817987129092216, -0.005787302274256945, -0.009513536468148232, -0.01888391375541687, 0.006778006907552481, -0.03996298462152481, 0.07906286418437958, -0.04934483394026756, 0.002534460509195924, -0.0013140749651938677, 0.009739372879266739, -0.018827378749847412, 0.09211372584104538, 0.02701706625521183, 0.010106097906827927, 0.013180524110794067, 0.05087077245116234, -0.029946329072117805, 0.012760140001773834, -0.012855489738285542, 0.008200105279684067, 0.010300644673407078, -0.03887227177619934, 0.087727852165699, -0.058080825954675674, -0.013879751786589622, -0.02160850167274475, 0.05208851397037506, -0.019328616559505463, 0.003200874198228121, 0.011286824941635132, -0.0064423275180161, -0.020448535680770874, -0.018702432513237, 0.026471124961972237, -0.033904481679201126, -0.010618172585964203, 0.025642720982432365, -0.0313863642513752, -0.011534960940480232, 0.007908572442829609, -0.04512445256114006, 0.02283351682126522, 0.009164620190858841, -0.03853495419025421, -0.0023934936616569757, -0.051996540278196335, -0.010612817481160164, 0.04071998968720436, -0.00025710684712976217, -0.024372100830078125, 0.045417264103889465, 0.07408653944730759, 0.003408790100365877, 0.08287716656923294, 0.04879200458526611, 0.050907570868730545, 0.010100091807544231, 0.0189394261687994, 0.013357345014810562, 0.010168148204684258, -0.035304151475429535, 0.002700807061046362, 0.05240805447101593, 0.020336272194981575, -0.012393375858664513, 0.07442007958889008, 0.06263931095600128, 0.04272599145770073, -0.02711469493806362, -0.004798205569386482, 0.013068271800875664, -0.02986450120806694, 0.00037538332981057465, 0.013057507574558258, -0.0968465507030487, -0.01798231340944767, 0.012189477682113647, 0.03501345217227936, -0.011081080883741379, -0.01745767891407013, -0.031983938068151474, -0.022064335644245148, 0.08485711365938187, -0.00343685457482934, -0.01456260122358799, 0.005010343622416258, -0.007688052020967007, 0.024608660489320755, -0.08292613178491592, 0.06397508084774017, 0.004690829198807478, 0.007913096807897091, 0.036022353917360306, 0.0006659068749286234, -0.01658443920314312, -0.018636764958500862, 0.024615516886115074, 0.02414553053677082, -0.06473634392023087, -0.009792412631213665, -0.05325734615325928, 0.01420909259468317, -0.039277199655771255, 0.023884374648332596, 0.05650574713945389, -0.02766450308263302, -0.024189328774809837, 0.029870592057704926, -0.032924309372901917, 0.019669799134135246, -0.018359094858169556, 0.018801331520080566, -0.02707786299288273, -0.020441895350813866, 0.004687879700213671, 0.002401745645329356, 0.06377092003822327, -0.04281676188111305, -0.0009759454987943172, 0.027595609426498413, 0.060424257069826126, 0.020918354392051697, -0.005486592650413513, -0.07342268526554108, -0.0526941753923893, 0.029353946447372437, -0.03353140875697136, -0.004850347992032766, 0.04146687686443329, -0.026472879573702812, 0.01267069112509489, -0.00446405028924346, 0.010773347690701485, 0.0038922347594052553, -0.01121237687766552, 0.0022828769870102406, -0.03537539765238762, 0.014624923467636108, -0.022506799548864365, 0.042016711086034775, -0.06862662732601166, -0.013486203737556934, 0.028224986046552658, 0.002582587068900466, 0.03553735837340355, -0.015434455126523972, -0.014334972016513348, 0.004324035253375769, 0.0053898063488304615, -0.07821765542030334, -0.004852551035583019, 0.058350931853055954, 0.05561845749616623, 0.04971618577837944, 0.00848366692662239, 0.019691959023475647, 0.04842359572649002, 0.016710050404071808, -0.036617863923311234, 0.046702999621629715, -0.023141957819461823, -0.014032494276762009, -0.004935077391564846, 0.017640557140111923, 0.05168399587273598, -0.008145507425069809, 0.003850450273603201, 0.07378315180540085, -0.025645993649959564, 0.06141502037644386, -0.005647069774568081, -0.03568589314818382, -0.012221494689583778, -0.019573714584112167, -0.010537681169807911, 0.00864588562399149, 0.020921990275382996, -0.04302797093987465, -0.020178483799099922, -0.028311334550380707, 0.009283555671572685, -0.02459694817662239, 0.024087658151984215, -0.004313063342124224, -0.005432801321148872, -0.013785123825073242, 0.024139773100614548, 0.05857562646269798, -0.03191772848367691, 0.012416687794029713, 0.016925476491451263, 0.006942843087017536, 0.006598522886633873, -0.0390513651072979, -0.007005717605352402, -8.729863475309685e-05, -0.04538340121507645, 0.004946487955749035, 0.029524078592658043, -0.034420207142829895, -0.004038716200739145, -0.008144606836140156, 0.027810344472527504, 0.0006213292945176363, 0.012055370956659317, 0.0009448639466427267, 0.0028901349287480116, -0.015417925082147121, 0.015339858829975128, -0.015768028795719147, 0.007656121626496315, 0.03895176202058792, -0.011974029242992401, -0.03488216921687126, 0.0021897954866290092, 0.008017251268029213, -0.023602457717061043, 0.042762432247400284, 0.017669593915343285, 0.02949250116944313, -0.004777966067194939, 0.01222960278391838, 0.04622475057840347, 0.004831378813832998, 0.005080347880721092, -0.03096405789256096, -0.024769319221377373, -0.05760312080383301, 0.006989185232669115, -0.02802259847521782, 0.0023186085745692253, 0.024799270555377007, -0.017475241795182228, -0.02310723438858986, -0.060716282576322556, -0.05141041800379753, -0.08543168753385544, -0.031041642650961876, 0.0007977064815349877, 0.00771120423451066, -0.014823099598288536, 0.017547423020005226, 0.028314258903265, -0.01559355203062296, -0.06470701843500137, 0.022269032895565033, -0.0012528174556791782, 0.003755373414605856, 0.02596835233271122, -0.014884010888636112, -0.016153506934642792, 0.012660589069128036, 0.051579441875219345, -0.026004411280155182, -0.010785234160721302, 0.024741262197494507, -0.006832896266132593, 0.006077229045331478, -0.018859494477510452, -0.04515213519334793, -0.015692224726080894, 0.02197863720357418, 0.027596719563007355, 0.013474834151566029, -0.06793735921382904, 0.03974653407931328, 0.029159316793084145, 0.009424426592886448, 0.036018598824739456, 0.06579939275979996, 0.0007757350686006248, 0.026988979429006577, -0.023084744811058044, 0.005725073162466288, -0.009881219826638699, 0.04924647510051727, -0.04520491138100624, 0.0036490694619715214, -0.0001661169226281345, 0.01312621496617794, 0.032258667051792145, -0.04117229953408241, -0.0017563514411449432, 0.005697781685739756, -0.004057110287249088, 0.00831490196287632, -0.01355521846562624, 0.017786549404263496, 0.019952373579144478, -0.017415044829249382, 0.02924727275967598, 0.00436373008415103, -0.028496161103248596, -0.0003454386896919459, -0.028011614456772804, 0.030717402696609497, 0.029748037457466125, 0.0013926341198384762, -0.029566174373030663, 0.022096160799264908, -0.01822168380022049, -0.03222834691405296, 0.005902382079511881, -0.036765407770872116, 0.008787472732365131, -0.02015107311308384, 0.06050626561045647, -0.01662573777139187, -0.024680834263563156, 0.04925467446446419, 0.027687592431902885, -0.0594508983194828, -0.03355632722377777, -0.0018203321378678083, -0.013882149010896683, 0.03436409682035446, -0.006525041069835424, 0.02155226469039917, -0.068003810942173, 0.009525789879262447, -0.055456023663282394, -0.0006107259541749954, -0.056682657450437546, -0.03278589993715286, 0.039782535284757614, -0.0029560134280472994, 0.04285448044538498, 0.015113752335309982, -0.006758872419595718, -0.005822359584271908, -0.023287208750844002, 0.01268066093325615, 0.006393882911652327, 0.005613046232610941, -0.024748165160417557, 0.04511215165257454, 0.028768694028258324, 0.01948765106499195, 0.02212745137512684, -0.023323310539126396, 0.00483762426301837, -0.004555167630314827, 0.0469304658472538, 0.06336293369531631, 0.06120479479432106, 0.024199720472097397, -0.07068958878517151, -0.023261142894625664, -0.03503222018480301, 0.07099641114473343, 0.07402566820383072, -0.022733237594366074, -0.013286171481013298, 0.013622286729514599, -0.021212313324213028, -0.022044561803340912, -0.005962895229458809, -0.033503927290439606, -0.02475627325475216, -0.042249567806720734, 0.02494623139500618, -0.0562412291765213, -0.09909646958112717, -0.03917242959141731, -0.021050311625003815, -0.006634335964918137, -0.020698025822639465, -0.003643974894657731, -0.027524033561348915, -0.031454574316740036, -0.06287893652915955, 0.005579663440585136, -0.015219196677207947, 0.0010854346910491586, 0.03304031863808632, -0.011660171672701836, -0.030525514855980873, 0.00619041221216321, 0.028455788269639015, 0.04060158133506775, -0.036002323031425476, 0.09819784760475159, 0.001239479286596179, 0.0327460952103138, -0.05798015370965004, -0.03414849564433098, 0.033689990639686584, -0.03496948629617691]\n"
     ]
    }
   ],
   "source": [
    "embedding_model = loader.load_embeddings()\n",
    "print(f\"Embedding Model Loaded: {embedding_model}\")\n",
    "result = embedding_model.embed_query(\"Hello, how are you?\")\n",
    "print(f\"Embedding Result: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a02b2cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableMap, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b44679a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "documents = [\n",
    "    Document(page_content=\"The Earth is the third planet from the Sun and the only known planet to support life. It has a diverse climate, ranging from arctic to tropical zones, and supports ecosystems across seven continents and five oceans.\"),\n",
    "\n",
    "    Document(page_content=\"The Industrial Revolution, beginning in the 18th century, drastically transformed human societies by shifting from manual labor to machine-based manufacturing, leading to urbanization and economic expansion globally.\"),\n",
    "\n",
    "    Document(page_content=\"The United Nations, established in 1945 after World War II, is an international organization founded to promote peace, security, human rights, and cooperation among countries. It has 193 member states.\"),\n",
    "\n",
    "    Document(page_content=\"The global economy is an interconnected system involving trade, investment, and financial flows across countries. Major players include the United States, China, the European Union, and emerging markets like India and Brazil.\"),\n",
    "\n",
    "    Document(page_content=\"Climate change refers to long-term shifts in temperatures and weather patterns. It is largely driven by human activities like burning fossil fuels, deforestation, and industrial emissions, leading to global warming and sea level rise.\"),\n",
    "\n",
    "    Document(page_content=\"Democracy is a political system in which citizens exercise power by voting. Modern democracies typically have institutions for free elections, rule of law, freedom of expression, and checks and balances.\"),\n",
    "\n",
    "    Document(page_content=\"The Internet has revolutionized communication, commerce, and education worldwide. Originating from military research in the 1960s, it now connects over 5 billion people, enabling instant global information exchange.\"),\n",
    "\n",
    "    Document(page_content=\"Renewable energy sources like solar, wind, hydro, and geothermal are critical for a sustainable future. They offer alternatives to fossil fuels, reducing carbon emissions and reliance on finite resources.\"),\n",
    "\n",
    "    Document(page_content=\"The World Health Organization (WHO) is a UN agency focused on global health issues. It coordinates international efforts to monitor diseases, set health standards, and respond to pandemics like COVID-19.\"),\n",
    "\n",
    "    Document(page_content=\"Globalization is the process of increasing interaction and integration among people, companies, and governments worldwide. It has led to greater economic growth but also raised concerns about inequality and cultural homogenization.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3b98100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "# Chroma vector DB with persistent storage\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=\"./chroma_db\"  # Disk path for persistence\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "efc156e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rv/z_0yz9v11p94xxdj7r_1w4vc0000gn/T/ipykernel_28942/1854114808.py:2: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vector_store.persist()\n"
     ]
    }
   ],
   "source": [
    "# Optional: Persist manually (though auto-persistence happens internally)\n",
    "vector_store.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b009ec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c631f36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Use the following context to answer the question.\n",
    "    If you don't know the answer, just say you don't know. Don't try to make up an answer.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5627089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCEL RAG Chain step-by-step\n",
    "rag_chain = (\n",
    "    RunnableMap({\n",
    "        \"context\": retriever | (lambda docs: \"\\n\\n\".join([doc.page_content for doc in docs])),\n",
    "        \"question\": RunnablePassthrough()\n",
    "    })\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74c936f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap with cache\n",
    "RAG_Cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b212ed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def cached_rag_chain(query):\n",
    "    start_time = time.time()\n",
    "    if RAG_Cache.get(query):\n",
    "        print(\"***CACHE HIT***\")\n",
    "        end_time = time.time()\n",
    "        elapsed = end_time - start_time\n",
    "        print(f\"EXECUTION TIME: {elapsed:.2f} seconds\")\n",
    "        return RAG_Cache.get(query)\n",
    "    else:\n",
    "        print(\"***CACHE MISS – EXECUTING MODEL***\")\n",
    "        start_time = time.time()\n",
    "        response = llm.invoke(query)\n",
    "        end_time = time.time()\n",
    "        elapsed = end_time - start_time\n",
    "        print(f\"EXECUTION TIME: {elapsed:.2f} seconds\")\n",
    "        RAG_Cache[query] = response\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a19492bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***CACHE MISS – EXECUTING MODEL***\n",
      "EXECUTION TIME: 9.82 seconds\n",
      "content=\"Okay, let's break down the Japanese economy in 2024 and its relationship with North Korea.\\n\\n**Japan's Economy in 2024: Overview**\\n\\nAs of late 2024, Japan's economy is navigating a complex landscape. Here's a summary of key aspects:\\n\\n*   **Growth:** Japan's economic growth is expected to be modest. The IMF and other organizations project growth in the range of 0.5-1% for 2024. This is influenced by factors like global economic conditions, domestic demand, and government policies.\\n*   **Inflation:** Japan has been experiencing inflation, a departure from its long history of deflation. The Bank of Japan (BOJ) has been closely monitoring this, and there have been some adjustments to monetary policy, including ending negative interest rates. The goal is to achieve sustainable inflation driven by wage growth and demand, rather than just cost-push factors.\\n*   **Monetary Policy:** The Bank of Japan (BOJ) has historically maintained an ultra-loose monetary policy, including negative interest rates and quantitative easing. In 2024, the BOJ has started to make adjustments, including ending negative interest rates, but is expected to proceed cautiously.\\n*   **Government Debt:** Japan has one of the highest levels of government debt in the world. Managing this debt burden is a significant challenge.\\n*   **Demographics:** Japan's aging and shrinking population continues to be a major headwind. It leads to labor shortages, reduced domestic demand, and increased social security costs.\\n*   **Key Sectors:**\\n    *   **Manufacturing:** Remains important, particularly in high-tech areas like automobiles, electronics, and machinery.\\n    *   **Services:** A growing sector, including tourism, finance, and healthcare.\\n    *   **Technology:** Japan is investing in areas like AI, robotics, and renewable energy.\\n*   **Trade:** Japan is a major trading nation. Key trading partners include the United States, China, South Korea, and other Asian countries.\\n*   **Challenges:**\\n    *   **Global Economic Slowdown:** A slowdown in the global economy could negatively impact Japan's exports.\\n    *   **Geopolitical Risks:** Tensions in the region, including those related to North Korea, can create uncertainty.\\n    *   **Energy Security:** Japan relies heavily on imported energy, making it vulnerable to price fluctuations.\\n    *   **Structural Reforms:** There is ongoing debate about the need for further structural reforms to boost productivity and address demographic challenges.\\n\\n**Japan's Relationship with North Korea in 2024**\\n\\nThe relationship between Japan and North Korea remains tense and complex. Here's a breakdown:\\n\\n*   **Historical Issues:**\\n    *   **Japanese Abductions:** One of the most sensitive issues is the abduction of Japanese citizens by North Korean agents in the 1970s and 1980s. Japan demands the return of all abductees and a full accounting of what happened to them. This issue is a major obstacle to any improvement in relations.\\n    *   **Colonial Past:** Japan's colonial rule over Korea (1910-1945) continues to cast a shadow on relations.\\n*   **Security Concerns:**\\n    *   **North Korea's Nuclear and Missile Programs:** Japan views North Korea's nuclear weapons and ballistic missile programs as a direct threat to its security. North Korean missile tests, especially those that fly over or near Japan, are met with strong condemnation.\\n    *   **Military Posture:** Japan is concerned about North Korea's military posture and its potential for aggression.\\n*   **Sanctions:**\\n    *   **Japan has imposed its own sanctions on North Korea, in addition to those imposed by the United Nations.** These sanctions are aimed at pressuring North Korea to abandon its nuclear and missile programs.\\n*   **Diplomacy:**\\n    *   **There is very little direct dialogue between Japan and North Korea.** Most communication occurs through international forums or via intermediaries.\\n    *   **Japan has stated its willingness to engage in dialogue with North Korea, but only if North Korea takes concrete steps towards denuclearization and addresses the abduction issue.**\\n*   **Current Situation (Late 2024):**\\n    *   **Tensions remain high.** North Korea continues to conduct missile tests, and Japan remains vigilant.\\n    *   **There is little sign of any breakthrough in relations.** The abduction issue remains a major sticking point.\\n    *   **Japan is working closely with the United States and South Korea to coordinate its response to North Korea.**\\n*   **Potential Future Developments:**\\n    *   **Any significant change in the situation would likely depend on North Korea's actions.** If North Korea were to take verifiable steps towards denuclearization and address the abduction issue, it could open the door to improved relations.\\n    *   **Changes in the regional geopolitical landscape could also affect the relationship.**\\n\\n**In summary:** Japan's economy in 2024 is facing challenges related to demographics, global economic conditions, and inflation. Its relationship with North Korea remains strained due to historical issues, security concerns, and North Korea's nuclear and missile programs. There is little prospect of a significant improvement in relations in the near term unless North Korea takes concrete steps to address Japan's concerns.\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run--b11d0926-7656-4794-9f17-482c3d83d306-0' usage_metadata={'input_tokens': 16, 'output_tokens': 1129, 'total_tokens': 1145, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "query = \"what is japan economy in 2024 and relation with north korea?\"\n",
    "response = cached_rag_chain(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e59ca01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***CACHE MISS – EXECUTING MODEL***\n",
      "EXECUTION TIME: 6.74 seconds\n",
      "content=\"LangChain is a framework designed to simplify the development of applications powered by large language models (LLMs). It provides tools, components, and interfaces to connect LLMs to various data sources and other computational resources, enabling developers to build more complex and sophisticated AI applications.\\n\\nHere's a breakdown of what makes LangChain useful:\\n\\n**Key Features and Components:**\\n\\n*   **LLM Integration:** LangChain provides a standardized interface for interacting with various LLMs, including OpenAI's GPT models, Cohere, Hugging Face models, and more. This allows you to easily switch between models or experiment with different LLMs without significantly altering your code.\\n\\n*   **Data Connection:**  A core strength of LangChain is its ability to connect LLMs to external data sources. This allows you to ground the LLM's responses in real-world information, making them more accurate and relevant.  It supports connections to:\\n    *   **Document Loaders:**  Load data from various file formats (PDFs, text files, websites, etc.) and databases.\\n    *   **Vector Databases:**  Store and retrieve embeddings of your data, enabling semantic search and retrieval.  Popular vector databases like Chroma, Pinecone, and FAISS are supported.\\n\\n*   **Chains:**  Chains are sequences of calls to LLMs or other utilities. They allow you to create more complex workflows by chaining together multiple operations.  Examples include:\\n    *   **Simple Sequential Chains:** Execute calls in a predefined order.\\n    *   **Router Chains:** Dynamically route calls to different chains based on input.\\n    *   **LLMMathChain:**  Combines an LLM with a calculator to answer questions that require mathematical reasoning.\\n\\n*   **Agents:** Agents use an LLM to determine which actions to take.  They have access to a set of tools (e.g., search engines, calculators, databases) and use the LLM to decide which tool to use and how to use it to achieve a specific goal.  This allows for more autonomous and flexible behavior.\\n\\n*   **Memory:**  LangChain provides mechanisms for adding memory to LLMs. This allows the LLM to remember previous interactions and use that information to inform future responses.  Different types of memory are supported, including:\\n    *   **ConversationBufferMemory:** Stores the entire conversation history.\\n    *   **ConversationSummaryMemory:** Summarizes the conversation history to save space.\\n    *   **ConversationBufferWindowMemory:** Stores a limited number of recent interactions.\\n\\n*   **Callbacks:**  Callbacks allow you to hook into different stages of the LangChain execution process. This can be useful for logging, monitoring, and debugging.\\n\\n**Why Use LangChain?**\\n\\n*   **Abstraction and Standardization:** LangChain provides a consistent interface for working with different LLMs and data sources, reducing the complexity of building LLM-powered applications.\\n*   **Rapid Prototyping:**  The framework's modular components and pre-built chains make it easier to quickly prototype and iterate on new ideas.\\n*   **Extensibility:**  LangChain is designed to be extensible, allowing you to create custom components and integrate with other tools and services.\\n*   **Community Support:**  LangChain has a large and active community, providing ample resources and support for developers.\\n\\n**Use Cases:**\\n\\n*   **Question Answering:**  Build applications that can answer questions based on a knowledge base of documents.\\n*   **Chatbots:**  Create conversational AI agents that can engage in natural language conversations.\\n*   **Text Summarization:**  Summarize large documents or articles.\\n*   **Code Generation:**  Generate code from natural language descriptions.\\n*   **Data Analysis:**  Analyze data and generate insights using LLMs.\\n*   **Agent-based Systems:**  Develop autonomous agents that can perform tasks using a variety of tools.\\n\\n**In summary, LangChain is a powerful framework that simplifies the development of LLM-powered applications by providing tools for connecting LLMs to data, creating complex workflows, and adding memory and reasoning capabilities.** It's a valuable resource for developers looking to build sophisticated AI applications that go beyond simple text generation.\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run--3b6fe9d3-3634-40f1-898c-b378a4011ab8-0' usage_metadata={'input_tokens': 6, 'output_tokens': 874, 'total_tokens': 880, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "query = \"what is langchain framework?\"\n",
    "response = cached_rag_chain(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a6ae0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***CACHE MISS – EXECUTING MODEL***\n",
      "EXECUTION TIME: 4.53 seconds\n",
      "content=\"The United Nations was established in 1945 after World War II for a multitude of crucial reasons, all stemming from the desire to prevent another global conflict and foster international cooperation. Here's a breakdown of the key motivations:\\n\\n*   **Preventing Future Wars:** The primary and overarching goal was to prevent another devastating world war like World War II. The war had caused unprecedented death, destruction, and suffering. The UN was designed to provide a forum for nations to discuss their differences, resolve disputes peacefully, and collectively address threats to international peace and security.\\n\\n*   **Replacing the League of Nations:** The League of Nations, established after World War I, had failed to prevent World War II. It was seen as weak and ineffective, lacking the necessary authority and support from major powers. The UN was intended to be a stronger, more effective organization with broader membership and a more robust mandate.\\n\\n*   **Promoting International Cooperation:** The UN aimed to foster cooperation among nations on a wide range of issues, including economic development, social progress, human rights, and environmental protection. The belief was that by working together, nations could address global challenges more effectively and create a more stable and prosperous world.\\n\\n*   **Upholding Human Rights:** The horrors of the Holocaust and other atrocities committed during World War II underscored the importance of protecting human rights. The UN Charter explicitly affirms the importance of human rights and fundamental freedoms for all, without distinction as to race, sex, language, or religion. The Universal Declaration of Human Rights, adopted in 1948, further elaborated on these rights.\\n\\n*   **Establishing International Law:** The UN aimed to establish a framework of international law to govern relations between states and promote a rules-based international order. This included the development of treaties, conventions, and other legal instruments to address issues such as the use of force, the treatment of prisoners of war, and the protection of civilians in armed conflict.\\n\\n*   **Promoting Economic and Social Development:** The UN recognized that poverty, inequality, and lack of opportunity could contribute to instability and conflict. Therefore, it sought to promote economic and social development in developing countries through technical assistance, financial aid, and other programs.\\n\\n*   **Decolonization:** Following World War II, many colonies sought independence. The UN played a significant role in supporting decolonization efforts, helping former colonies transition to self-government and independence.\\n\\nIn summary, the United Nations was established as a direct response to the devastation of World War II, with the core objectives of preventing future wars, promoting international cooperation, upholding human rights, establishing international law, and fostering economic and social development. It was intended to be a more effective and comprehensive organization than its predecessor, the League of Nations, and to provide a framework for addressing global challenges collectively.\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run--16570bd4-239d-4b40-9549-ea8a1fec3221-0' usage_metadata={'input_tokens': 16, 'output_tokens': 570, 'total_tokens': 586, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "query = \"Why United Nations, established in 1945 after World War II?\"\n",
    "response = cached_rag_chain(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008817ed",
   "metadata": {},
   "source": [
    "## Cache Using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f511a1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import InMemoryCache\n",
    "from langchain.globals import set_llm_cache\n",
    "from typing import Any, Dict, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "10cb8194",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DebuggableCache(InMemoryCache):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._cache: Dict[Tuple[str, str], Any] = {}\n",
    "\n",
    "    def lookup(self, prompt: str, llm_string: str):\n",
    "        return self._cache.get((prompt, llm_string))\n",
    "\n",
    "    def update(self, prompt: str, llm_string: str, return_val: Any):\n",
    "        self._cache[(prompt, llm_string)] = return_val\n",
    "\n",
    "    def view_cache(self):  # this is our custom method\n",
    "        return self._cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e663ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbg_cache = DebuggableCache()\n",
    "set_llm_cache(dbg_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06147a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9d480168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response: content='The capital of France is **Paris**.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run--559d5eb4-8d45-467e-8f8f-2250eae8dd87-0' usage_metadata={'input_tokens': 7, 'output_tokens': 9, 'total_tokens': 16, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(\"LLM Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "70ec8e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cache Contents:\n",
      "Prompt: [{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"messages\", \"HumanMessage\"], \"kwargs\": {\"content\": \"What is the capital of France?\", \"type\": \"human\"}}] | Cached Output: [ChatGeneration(text='The capital of France is **Paris**.', generation_info={'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, message=AIMessage(content='The capital of France is **Paris**.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--559d5eb4-8d45-467e-8f8f-2250eae8dd87-0', usage_metadata={'input_tokens': 7, 'output_tokens': 9, 'total_tokens': 16, 'input_token_details': {'cache_read': 0}}))]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCache Contents:\")\n",
    "for k, v in dbg_cache.view_cache().items():\n",
    "    print(f\"Prompt: {k[0]} | Cached Output: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cbe0b148",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a9e79f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cache Contents:\n",
      "Prompt: [{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"messages\", \"HumanMessage\"], \"kwargs\": {\"content\": \"What is the capital of France?\", \"type\": \"human\"}}] | Cached Output: [ChatGeneration(text='The capital of France is **Paris**.', generation_info={'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, message=AIMessage(content='The capital of France is **Paris**.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--559d5eb4-8d45-467e-8f8f-2250eae8dd87-0', usage_metadata={'input_tokens': 7, 'output_tokens': 9, 'total_tokens': 16, 'input_token_details': {'cache_read': 0}}))]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCache Contents:\")\n",
    "for k, v in dbg_cache.view_cache().items():\n",
    "    print(f\"Prompt: {k[0]} | Cached Output: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "09dbe572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response: content='The capital of India is **New Delhi**.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run--dd0e13ca-edc6-447f-8133-3eea989c91e2-0' usage_metadata={'input_tokens': 7, 'output_tokens': 10, 'total_tokens': 17, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"What is the capital of india?\")\n",
    "print(\"LLM Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f3ca1036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cache Contents:\n",
      "Prompt: [{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"messages\", \"HumanMessage\"], \"kwargs\": {\"content\": \"What is the capital of France?\", \"type\": \"human\"}}] | Cached Output: [ChatGeneration(text='The capital of France is **Paris**.', generation_info={'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, message=AIMessage(content='The capital of France is **Paris**.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--559d5eb4-8d45-467e-8f8f-2250eae8dd87-0', usage_metadata={'input_tokens': 7, 'output_tokens': 9, 'total_tokens': 16, 'input_token_details': {'cache_read': 0}}))]\n",
      "Prompt: [{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"messages\", \"HumanMessage\"], \"kwargs\": {\"content\": \"What is the capital of india?\", \"type\": \"human\"}}] | Cached Output: [ChatGeneration(text='The capital of India is **New Delhi**.', generation_info={'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, message=AIMessage(content='The capital of India is **New Delhi**.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--dd0e13ca-edc6-447f-8133-3eea989c91e2-0', usage_metadata={'input_tokens': 7, 'output_tokens': 10, 'total_tokens': 17, 'input_token_details': {'cache_read': 0}}))]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCache Contents:\")\n",
    "for k, v in dbg_cache.view_cache().items():\n",
    "    print(f\"Prompt: {k[0]} | Cached Output: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1972e15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response: content='The capital of India is **New Delhi**.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run--dd0e13ca-edc6-447f-8133-3eea989c91e2-0' usage_metadata={'input_tokens': 7, 'output_tokens': 10, 'total_tokens': 17, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"What is the capital of india?\")\n",
    "print(\"LLM Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7707f243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response: content=\"## The Indelible Bond: Science, Mathematics, and the Pursuit of Understanding\\n\\nScience, in its broadest sense, is the systematic pursuit of knowledge about the natural world through observation, experimentation, and analysis. It is a dynamic and ever-evolving process, driven by curiosity and a desire to understand the intricate workings of the universe. From the smallest subatomic particles to the vast expanse of galaxies, science seeks to unravel the mysteries that surround us, providing explanations and predictions that shape our understanding of reality. At the heart of this endeavor lies a crucial and indispensable tool: mathematics. Mathematics provides the language, the structure, and the logical framework upon which scientific theories are built, tested, and refined. The relationship between science and mathematics is not merely one of utility; it is a symbiotic partnership, a fundamental and inseparable bond that fuels scientific progress and deepens our comprehension of the cosmos.\\n\\nThe importance of mathematics in science stems from its ability to provide a precise and unambiguous language for describing natural phenomena. Unlike everyday language, which is often vague and subjective, mathematics offers a rigorous and objective system for expressing relationships and quantities. This precision is essential for formulating scientific laws and theories that can be tested and verified through experimentation. For example, Newton's law of universal gravitation, expressed mathematically as F = Gm1m2/r², provides a concise and accurate description of the force of attraction between any two objects with mass. This equation, built upon mathematical concepts like force, mass, and distance, allows scientists to predict the motion of celestial bodies with remarkable accuracy. Without the language of mathematics, such a precise and powerful formulation would be impossible.\\n\\nFurthermore, mathematics provides the tools for analyzing data and drawing meaningful conclusions from scientific experiments. Statistical methods, for instance, are crucial for determining the significance of experimental results and distinguishing between genuine effects and random variations. By applying mathematical models to experimental data, scientists can identify patterns, trends, and correlations that might otherwise go unnoticed. This allows them to test hypotheses, refine theories, and make predictions about future events. In fields like medicine, statistical analysis is essential for evaluating the effectiveness of new treatments and identifying risk factors for diseases. In climate science, mathematical models are used to analyze vast amounts of data and predict the effects of climate change on the planet. In each of these cases, mathematics provides the analytical framework necessary for extracting valuable insights from complex data sets.\\n\\nBeyond its role in data analysis, mathematics is also essential for developing theoretical models that explain and predict natural phenomena. These models, often expressed as mathematical equations or algorithms, provide a simplified representation of reality that allows scientists to explore the consequences of different assumptions and make predictions about future behavior. For example, the Standard Model of particle physics, a highly successful theory that describes the fundamental building blocks of matter and their interactions, is based on complex mathematical concepts like quantum field theory and group theory. Similarly, Einstein's theory of general relativity, which describes gravity as a curvature of spacetime, relies heavily on differential geometry and tensor calculus. These theoretical models, built upon the foundation of mathematics, provide a framework for understanding the fundamental laws of nature and making predictions about the behavior of the universe.\\n\\nThe importance of mathematics in science extends to virtually every scientific discipline. In physics, mathematics is used to describe the motion of objects, the behavior of electromagnetic fields, and the properties of quantum particles. In chemistry, mathematics is used to model chemical reactions, predict the properties of molecules, and analyze spectroscopic data. In biology, mathematics is used to model population dynamics, analyze genetic sequences, and understand the spread of diseases. In engineering, mathematics is used to design bridges, build airplanes, and develop new technologies. In each of these fields, mathematics provides the essential tools for understanding, predicting, and manipulating the natural world.\\n\\nConsider the field of astrophysics. Understanding the formation and evolution of stars, galaxies, and the universe itself relies heavily on mathematical models. Equations of stellar structure, derived from principles of physics and expressed in mathematical form, allow astronomers to predict the temperature, density, and luminosity of stars. Mathematical models of galaxy formation, based on the laws of gravity and fluid dynamics, help us understand how galaxies evolve over billions of years. Even the Big Bang theory, the prevailing cosmological model for the origin of the universe, is based on mathematical solutions to Einstein's equations of general relativity. Without the power of mathematics, our understanding of the cosmos would be severely limited.\\n\\nSimilarly, in the field of climate science, mathematical models are used to simulate the Earth's climate system and predict the effects of greenhouse gas emissions. These models, which incorporate complex interactions between the atmosphere, oceans, land surface, and ice sheets, rely on sophisticated mathematical techniques like numerical analysis and computational fluid dynamics. By running these models on powerful computers, scientists can explore different scenarios for future climate change and assess the potential impacts on human societies and ecosystems. The accuracy and reliability of these models depend critically on the underlying mathematical framework and the ability to solve complex equations that describe the behavior of the climate system.\\n\\nThe relationship between science and mathematics is not a one-way street. While science relies heavily on mathematics, mathematics also benefits from its interaction with science. Scientific problems often inspire new mathematical concepts and techniques. For example, the development of calculus was motivated by the need to solve problems in physics, such as determining the velocity and acceleration of moving objects. Similarly, the development of probability theory was driven by the desire to understand games of chance and analyze statistical data. The interaction between science and mathematics is a dynamic and mutually beneficial process that drives progress in both fields.\\n\\nFurthermore, the application of mathematics to scientific problems often leads to new insights and discoveries that would not have been possible otherwise. By translating scientific questions into mathematical language, scientists can gain a deeper understanding of the underlying principles and identify new relationships that might have been overlooked. For example, the application of mathematical techniques to the study of chaos theory has revealed that even simple deterministic systems can exhibit complex and unpredictable behavior. This has had profound implications for our understanding of a wide range of phenomena, from weather patterns to stock market fluctuations.\\n\\nThe importance of mathematics in science is not limited to theoretical research. Mathematics is also essential for practical applications of science and technology. Engineers use mathematical principles to design and build everything from bridges and buildings to airplanes and computers. Computer scientists use mathematical algorithms to develop software, analyze data, and create artificial intelligence systems. Medical researchers use mathematical models to develop new drugs, diagnose diseases, and improve patient care. In each of these areas, mathematics provides the essential tools for solving real-world problems and improving the quality of life.\\n\\nIn conclusion, the relationship between science and mathematics is a fundamental and inseparable bond that fuels scientific progress and deepens our comprehension of the cosmos. Mathematics provides the language, the structure, and the logical framework upon which scientific theories are built, tested, and refined. It is essential for analyzing data, developing theoretical models, and making predictions about the natural world. The importance of mathematics in science extends to virtually every scientific discipline, from physics and chemistry to biology and engineering. The interaction between science and mathematics is a dynamic and mutually beneficial process that drives progress in both fields. As we continue to explore the mysteries of the universe, the power of mathematics will remain an indispensable tool for unlocking new knowledge and shaping our understanding of reality. The future of scientific discovery is inextricably linked to the continued development and application of mathematical principles. Therefore, fostering a strong foundation in mathematics is crucial for nurturing the next generation of scientists and ensuring continued progress in our quest to understand the world around us. The pursuit of scientific knowledge and the mastery of mathematical tools are two sides of the same coin, essential for unlocking the secrets of the universe and building a better future for humanity.\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run--409b083f-0835-43dd-b29a-c917ace9e692-0' usage_metadata={'input_tokens': 21, 'output_tokens': 1565, 'total_tokens': 1586, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"give me 1000 lines of essay on science and give the importance of it regarding mathematics?\")\n",
    "print(\"LLM Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f9df7df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Okay, here's a joke about LangChain and ECS:\n",
      "\n",
      "Why did the LangChain agent refuse to deploy on ECS?\n",
      "\n",
      "Because it kept saying, \"I can't find the context! I need more environment variables!  I'm pretty sure my memory is leaking, and I'm not even sure if I'm running in a container or just hallucinating the whole thing!\"\n",
      "\n",
      "...Turns out, the ECS task definition was missing the necessary IAM role to access the S3 bucket where the agent's knowledge base was stored.  It was a classic case of \"no context, no content!\"\n",
      "Token Usage Stats: Tokens Used: 136\n",
      "\tPrompt Tokens: 9\n",
      "\t\tPrompt Tokens Cached: 0\n",
      "\tCompletion Tokens: 127\n",
      "\t\tReasoning Tokens: 0\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "with get_openai_callback() as cb:\n",
    "    response = llm.invoke(\"Tell me a joke about LangChain and ECS\")\n",
    "    print(\"Response:\", response.content)\n",
    "    print(\"Token Usage Stats:\", cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "216b5232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What is Retrieval-Augmented Generation?\n",
      "A: Retrieval-Augmented Generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by allowing them to access and incorporate information from external knowledge sources during the generation process.  Think of it as giving an LLM a textbook or a library to consult before answering a question.\n",
      "\n",
      "Here's a breakdown of the key components and how it works:\n",
      "\n",
      "**1. Retrieval:**\n",
      "\n",
      "*   **Knowledge Source:** This is the external database or repository containing the information the LLM will use. It could be:\n",
      "    *   A collection of documents (PDFs, text files, web pages)\n",
      "    *   A knowledge graph\n",
      "    *   A database\n",
      "    *   A vector database (more on this below)\n",
      "*   **Query Generation:** When a user asks a question or provides a prompt, the RAG system generates a query to search the knowledge source. This query is often a vector representation of the user's input, designed to capture the semantic meaning.\n",
      "*   **Information Retrieval:** The query is used to retrieve relevant information from the knowledge source.  This is often done using techniques like:\n",
      "    *   **Semantic Search:**  Finding information based on the meaning of the query, rather than just keyword matching.  This is where vector databases come in handy.\n",
      "    *   **Keyword Search:**  A more traditional approach, but often less effective than semantic search.\n",
      "*   **Vector Databases:** These are specialized databases designed to store and efficiently search high-dimensional vectors (embeddings).  They are crucial for semantic search in RAG systems because they allow for fast retrieval of information that is semantically similar to the query.\n",
      "\n",
      "**2. Generation:**\n",
      "\n",
      "*   **Contextualization:** The retrieved information is combined with the original user prompt to create a richer context for the LLM.  This context provides the LLM with the necessary knowledge to answer the question accurately and comprehensively.\n",
      "*   **Text Generation:** The LLM uses the combined context (user prompt + retrieved information) to generate a response.  It leverages its pre-trained knowledge and reasoning abilities, now augmented with the external information, to produce a more informed and relevant answer.\n",
      "\n",
      "**In simpler terms:**\n",
      "\n",
      "1.  **User asks a question.**\n",
      "2.  **The RAG system searches a knowledge base for relevant information.**\n",
      "3.  **The RAG system combines the question with the retrieved information.**\n",
      "4.  **The LLM uses this combined information to generate an answer.**\n",
      "\n",
      "**Why is RAG important?**\n",
      "\n",
      "*   **Improved Accuracy:**  LLMs are prone to generating incorrect or hallucinated information, especially when dealing with specialized or up-to-date topics. RAG helps mitigate this by grounding the LLM's responses in verifiable external knowledge.\n",
      "*   **Reduced Hallucinations:** By providing a source of truth, RAG reduces the likelihood of the LLM making things up.\n",
      "*   **Up-to-Date Information:** LLMs are trained on massive datasets, but these datasets are often static and outdated. RAG allows LLMs to access and incorporate the latest information from external sources, keeping their responses current.\n",
      "*   **Explainability and Traceability:** RAG systems can often provide citations or links to the sources used to generate the response, making it easier to verify the information and understand the reasoning behind the answer.  This increases trust and transparency.\n",
      "*   **Customization and Domain Specificity:** RAG allows you to tailor an LLM to a specific domain or knowledge base.  For example, you could create a RAG system that uses a company's internal documentation to answer employee questions.\n",
      "*   **Cost-Effective:**  Fine-tuning an LLM on a specific dataset can be expensive and time-consuming. RAG offers a more cost-effective way to adapt an LLM to a specific task or domain.\n",
      "\n",
      "**Example:**\n",
      "\n",
      "Let's say you ask an LLM: \"What is the current price of Bitcoin?\"\n",
      "\n",
      "*   **Without RAG:** The LLM might rely on its outdated training data and provide an inaccurate price.  It might even hallucinate a price.\n",
      "*   **With RAG:**\n",
      "    1.  The RAG system generates a query like \"current Bitcoin price\".\n",
      "    2.  It searches a real-time financial data API or website.\n",
      "    3.  It retrieves the current price of Bitcoin from the API.\n",
      "    4.  It combines the user's question with the retrieved price.\n",
      "    5.  The LLM generates a response like: \"The current price of Bitcoin is $27,000 (as of [date and time]).\"\n",
      "\n",
      "**In summary, RAG is a powerful technique that combines the strengths of information retrieval and language generation to create more accurate, reliable, and up-to-date LLM applications.** It's a key technology for building real-world applications that require access to external knowledge.\n",
      "\n",
      "Q: How does FAISS indexing work?\n",
      "A: FAISS (Facebook AI Similarity Search) is a library designed for efficient similarity search and clustering of dense vectors. It's particularly useful for large datasets where brute-force search becomes computationally expensive. Here's a breakdown of how FAISS indexing works, covering the key concepts and techniques:\n",
      "\n",
      "**1. The Problem: Nearest Neighbor Search**\n",
      "\n",
      "The core problem FAISS addresses is finding the *k* nearest neighbors (k-NN) to a query vector within a large dataset of vectors.  A naive approach would involve calculating the distance between the query vector and every vector in the dataset, which is O(N) complexity, where N is the number of vectors.  For millions or billions of vectors, this becomes impractical.\n",
      "\n",
      "**2. FAISS's Approach: Approximate Nearest Neighbor (ANN) Search**\n",
      "\n",
      "FAISS employs various techniques to perform *approximate* nearest neighbor search.  This means it might not always find the absolute closest neighbors, but it finds neighbors that are very close with high probability, and it does so much faster than brute-force.  The trade-off is accuracy for speed.\n",
      "\n",
      "**3. Key Concepts and Techniques:**\n",
      "\n",
      "*   **Vector Quantization:** This is a fundamental technique used in many FAISS indexes. It involves dividing the vector space into a set of discrete regions (cells or clusters) represented by their centroids (cluster centers).  Instead of comparing the query vector to every vector in the dataset, you compare it to the centroids and then only search within the most promising cells.\n",
      "\n",
      "*   **Index Types:** FAISS offers a wide variety of index types, each optimized for different scenarios (dataset size, vector dimensionality, accuracy requirements, memory constraints, etc.).  Here are some common ones:\n",
      "\n",
      "    *   **Flat Index (IndexFlatL2, IndexFlatIP):** This is the simplest index. It stores the vectors directly and performs a brute-force search.  It's accurate but slow for large datasets.  `L2` uses Euclidean distance, and `IP` uses inner product (cosine similarity).\n",
      "\n",
      "    *   **IVF (Inverted File Index):** This is a popular and versatile index. It combines vector quantization with an inverted index.\n",
      "        *   **Training:**  A set of vectors is used to train a quantizer (e.g., k-means).  The quantizer learns the centroids of the cells.\n",
      "        *   **Indexing:** Each vector in the dataset is assigned to the nearest cell (centroid).  An inverted index is created, mapping each cell ID to the list of vectors belonging to that cell.\n",
      "        *   **Searching:**\n",
      "            1.  The query vector is compared to the centroids of the cells.\n",
      "            2.  The `nprobe` closest cells are selected (a parameter you control).\n",
      "            3.  The query vector is then compared to all vectors within the selected cells.\n",
      "\n",
      "    *   **PQ (Product Quantization):** This technique divides the vector into subvectors and quantizes each subvector independently.  This allows for a more compact representation of the vectors.\n",
      "        *   **Training:** The vector space is divided into `m` subvectors.  For each subvector, a separate quantizer (e.g., k-means) is trained.\n",
      "        *   **Encoding:** Each vector is encoded by finding the nearest centroid for each of its subvectors.  The vector is then represented by the indices of these centroids.\n",
      "        *   **Distance Calculation:**  A lookup table is precomputed that stores the distances between all possible pairs of subvector centroids.  The distance between a query vector and an encoded vector is then calculated using this lookup table.\n",
      "\n",
      "    *   **HNSW (Hierarchical Navigable Small World):** This is a graph-based index that builds a multi-layer graph where each layer represents a different level of granularity.  It's known for its good performance and scalability.\n",
      "        *   **Construction:** Vectors are inserted into the graph one by one.  For each vector, connections are made to its nearest neighbors in the graph.  The graph is organized into layers, with higher layers containing fewer nodes and representing a coarser approximation of the vector space.\n",
      "        *   **Searching:** The search starts at the top layer of the graph and navigates down to the lower layers, following the connections to find the nearest neighbors of the query vector.\n",
      "\n",
      "    *   **Composite Indexes:** FAISS allows you to combine different index types to create more complex and optimized indexes.  For example, you can combine IVF with PQ (IVF + PQ) to get the benefits of both techniques.\n",
      "\n",
      "*   **Distance Metrics:** FAISS supports various distance metrics, including:\n",
      "\n",
      "    *   **L2 (Euclidean Distance):** The most common distance metric.\n",
      "    *   **Inner Product (IP):**  Used for cosine similarity.\n",
      "    *   **Jaccard Distance:**  Suitable for binary vectors.\n",
      "\n",
      "*   **Parameters:**  Each index type has parameters that control its behavior and performance.  These parameters need to be tuned based on the specific dataset and application.  Key parameters include:\n",
      "\n",
      "    *   `nlist` (IVF): The number of cells (clusters) in the inverted index.\n",
      "    *   `nprobe` (IVF): The number of cells to search during query time.  Increasing `nprobe` increases accuracy but also increases search time.\n",
      "    *   `m` (PQ): The number of subvectors in product quantization.\n",
      "    *   `efConstruction` (HNSW):  Controls the quality of the graph during construction.\n",
      "    *   `efSearch` (HNSW): Controls the search effort during query time.\n",
      "\n",
      "**4. Workflow:**\n",
      "\n",
      "1.  **Choose an Index Type:** Select the appropriate index type based on your dataset size, vector dimensionality, accuracy requirements, and memory constraints.\n",
      "2.  **Train the Index (if necessary):** Some index types (e.g., IVF, PQ) require training on a representative subset of your data.  This training step learns the quantization parameters.\n",
      "3.  **Add Vectors to the Index:** Add your vectors to the index.  This process involves assigning each vector to its corresponding cell or encoding it using the learned quantization parameters.\n",
      "4.  **Search the Index:**  Provide a query vector and the number of nearest neighbors you want to find.  FAISS will efficiently search the index and return the approximate nearest neighbors.\n",
      "\n",
      "**5. Code Example (Python):**\n",
      "\n",
      "```python\n",
      "import faiss\n",
      "import numpy as np\n",
      "\n",
      "# Example data\n",
      "d = 128  # Dimensionality of the vectors\n",
      "nb = 10000  # Number of database vectors\n",
      "nq = 100  # Number of query vectors\n",
      "np.random.seed(123)\n",
      "xb = np.random.random((nb, d)).astype('float32')\n",
      "xq = np.random.random((nq, d)).astype('float32')\n",
      "\n",
      "# Choose an index type (e.g., IVF64, Flat)\n",
      "nlist = 64  # Number of cells\n",
      "quantizer = faiss.IndexFlatL2(d)  # Use L2 distance\n",
      "index = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_L2)\n",
      "\n",
      "# Train the index\n",
      "index.train(xb)\n",
      "\n",
      "# Add vectors to the index\n",
      "index.add(xb)\n",
      "\n",
      "# Set the number of probes (cells to search)\n",
      "nprobe = 16\n",
      "index.nprobe = nprobe\n",
      "\n",
      "# Search the index\n",
      "k = 10  # Number of nearest neighbors to retrieve\n",
      "D, I = index.search(xq, k)  # D: distances, I: indices\n",
      "\n",
      "# Print the results\n",
      "print(I[:5])  # Print the indices of the nearest neighbors for the first 5 query vectors\n",
      "print(D[:5])  # Print the distances to the nearest neighbors for the first 5 query vectors\n",
      "```\n",
      "\n",
      "**6. Considerations:**\n",
      "\n",
      "*   **Memory Usage:**  Different index types have different memory footprints.  PQ indexes are generally more memory-efficient than IVF indexes.\n",
      "*   **Accuracy vs. Speed:**  There's a trade-off between accuracy and speed.  Increasing the number of probes (e.g., `nprobe` in IVF) or using a more complex index type will generally improve accuracy but also increase search time.\n",
      "*   **Parameter Tuning:**  Finding the optimal parameters for a given index type can be challenging.  It often requires experimentation and evaluation on a validation set.\n",
      "*   **Data Distribution:** The performance of FAISS indexes can be affected by the distribution of the data.  If the data is highly clustered, some index types may perform better than others.\n",
      "\n",
      "**In summary, FAISS provides a powerful set of tools for efficient similarity search. By using techniques like vector quantization and inverted indexes, it can significantly reduce the search time compared to brute-force methods, making it suitable for large-scale datasets.  Choosing the right index type and tuning its parameters are crucial for achieving optimal performance.**\n",
      "\n",
      "Q: What is the difference between fine-tuning and RAG?\n",
      "A: Both fine-tuning and Retrieval-Augmented Generation (RAG) are techniques used to improve the performance of Large Language Models (LLMs) by incorporating external knowledge. However, they differ significantly in how they achieve this and when they are most appropriate. Here's a breakdown of the key differences:\n",
      "\n",
      "**Fine-tuning:**\n",
      "\n",
      "*   **Mechanism:** Modifies the internal parameters (weights) of the LLM itself. It involves training the LLM on a new dataset, adjusting its existing knowledge and capabilities.\n",
      "*   **Knowledge Integration:**  Embeds new knowledge directly into the model's parameters. The model learns to associate specific inputs with desired outputs based on the training data.\n",
      "*   **Data Requirements:** Requires a substantial amount of labeled or high-quality data relevant to the specific task or domain.\n",
      "*   **Computational Cost:**  Can be computationally expensive, especially for large models, as it involves retraining the model.\n",
      "*   **Update Frequency:**  Less frequent. Fine-tuning is typically done when there's a significant change in the domain or task, or when the model's performance needs a substantial overhaul.\n",
      "*   **Use Cases:**\n",
      "    *   Adapting a general-purpose LLM to a specific domain (e.g., legal, medical, financial).\n",
      "    *   Improving the model's ability to follow specific instructions or generate text in a particular style.\n",
      "    *   Teaching the model new skills or tasks.\n",
      "*   **Pros:**\n",
      "    *   Can lead to significant improvements in performance for specific tasks.\n",
      "    *   The model becomes more efficient and self-contained after fine-tuning.\n",
      "    *   Can improve the model's ability to generalize to unseen data within the target domain.\n",
      "*   **Cons:**\n",
      "    *   Requires a large, high-quality dataset.\n",
      "    *   Can be computationally expensive and time-consuming.\n",
      "    *   Risk of overfitting to the training data, leading to poor performance on unseen data.\n",
      "    *   Can be difficult to update the model with new information after fine-tuning.\n",
      "    *   Catastrophic forgetting: The model might \"forget\" previously learned information.\n",
      "\n",
      "**Retrieval-Augmented Generation (RAG):**\n",
      "\n",
      "*   **Mechanism:**  Keeps the LLM's parameters fixed and augments its input with relevant information retrieved from an external knowledge source (e.g., a database, a collection of documents, or the internet).\n",
      "*   **Knowledge Integration:**  Retrieves relevant information at inference time and feeds it to the LLM as part of the input prompt. The LLM uses this retrieved information to generate a more informed and accurate response.\n",
      "*   **Data Requirements:**  Requires a well-structured and up-to-date knowledge source. The LLM itself doesn't need to be retrained.\n",
      "*   **Computational Cost:**  Less computationally expensive than fine-tuning, as it only involves retrieving information and feeding it to the LLM.\n",
      "*   **Update Frequency:**  More frequent. The knowledge source can be updated regularly without retraining the LLM.\n",
      "*   **Use Cases:**\n",
      "    *   Answering questions based on a specific document or knowledge base.\n",
      "    *   Generating summaries of documents.\n",
      "    *   Providing up-to-date information on rapidly changing topics.\n",
      "    *   Building chatbots that can access and use external knowledge.\n",
      "*   **Pros:**\n",
      "    *   Allows the LLM to access and use a vast amount of external knowledge.\n",
      "    *   Easy to update the knowledge source without retraining the LLM.\n",
      "    *   Reduces the risk of the LLM generating incorrect or outdated information.\n",
      "    *   More transparent and explainable, as the source of the information is known.\n",
      "*   **Cons:**\n",
      "    *   Performance depends on the quality and relevance of the retrieved information.\n",
      "    *   Can be slower than fine-tuning, as it involves retrieving information at inference time.\n",
      "    *   Requires a well-structured and maintained knowledge source.\n",
      "    *   The LLM might struggle to integrate the retrieved information effectively.\n",
      "\n",
      "**Here's a table summarizing the key differences:**\n",
      "\n",
      "| Feature          | Fine-tuning                               | RAG                                         |\n",
      "|-------------------|-------------------------------------------|---------------------------------------------|\n",
      "| **Mechanism**     | Modifies model parameters                | Augments input with retrieved information |\n",
      "| **Knowledge Integration** | Embeds knowledge into model parameters | Retrieves knowledge at inference time      |\n",
      "| **Data Requirements** | Large, labeled dataset                   | Well-structured knowledge source            |\n",
      "| **Computational Cost** | High                                      | Low                                         |\n",
      "| **Update Frequency** | Low                                       | High                                        |\n",
      "| **Knowledge Source** | Internal (model weights)                  | External (database, documents, etc.)        |\n",
      "| **Transparency**   | Lower                                     | Higher                                      |\n",
      "\n",
      "**In essence:**\n",
      "\n",
      "*   **Fine-tuning is like teaching the LLM a new subject in school.** It requires significant effort and time, but the LLM becomes an expert in that subject.\n",
      "*   **RAG is like giving the LLM a textbook to consult while answering questions.** It's faster and easier to update, but the LLM still needs to be able to understand and use the information in the textbook.\n",
      "\n",
      "**When to use which?**\n",
      "\n",
      "*   **Use Fine-tuning when:**\n",
      "    *   You need to adapt a general-purpose LLM to a specific domain or task.\n",
      "    *   You have a large, high-quality dataset for training.\n",
      "    *   You need the model to be efficient and self-contained.\n",
      "    *   The knowledge is relatively static.\n",
      "\n",
      "*   **Use RAG when:**\n",
      "    *   You need to access and use a vast amount of external knowledge.\n",
      "    *   The knowledge is constantly changing.\n",
      "    *   You need to provide up-to-date information.\n",
      "    *   You want to improve the transparency and explainability of the model's responses.\n",
      "    *   You don't have enough data to fine-tune effectively.\n",
      "\n",
      "**Can they be combined?**\n",
      "\n",
      "Yes!  It's becoming increasingly common to combine fine-tuning and RAG.  You might fine-tune a model to be better at understanding and utilizing retrieved information, or to be better at a specific task within a domain, and then use RAG to provide it with the most up-to-date and relevant information for that task.  This can lead to the best of both worlds: a model that is both knowledgeable and efficient.\n",
      "\n",
      "=== Token Usage Summary ===\n",
      "Total Tokens: 4346\n",
      "Prompt Tokens: 26\n",
      "Completion Tokens: 4320\n",
      "Total Cost (USD): $0.000000\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What is Retrieval-Augmented Generation?\",\n",
    "    \"How does FAISS indexing work?\",\n",
    "    \"What is the difference between fine-tuning and RAG?\"\n",
    "]\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    for q in questions:\n",
    "        answer = llm.invoke(q)\n",
    "        print(f\"\\nQ: {q}\\nA: {answer.content}\")\n",
    "\n",
    "    print(\"\\n=== Token Usage Summary ===\")\n",
    "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "    print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "    print(f\"Total Cost (USD): ${cb.total_cost:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cff0aee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
